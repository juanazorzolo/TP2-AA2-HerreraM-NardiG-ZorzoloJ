{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AZk6paD04pWD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Trabajo Práctico 2 AAII 2025 - Redes Recurrentes y Reinforcement Learning\n",
        "\n"
      ],
      "metadata": {
        "id": "tXqUA6MI4R41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEMA 1 -GRUPO N° 5\n",
        "\n",
        "Autores:\n",
        "\n",
        "- Herrera Morena (H-1187/8)\n",
        "\n",
        "- Nardi Gianella Belén (N-1277/7)\n",
        "\n",
        "- Zorzolo Rubio Juana (Z-1217/3)"
      ],
      "metadata": {
        "id": "6tdgVOCV0Kw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objetivo: construir un modelo de clasificación utilizando redes neuronales que pueda inferir con precisión el dígito correspondiente dado un clip de audio. Se deben entrenar y evaluar modelos utilizando técnicas adecuadas de validación y métricas de evaluación de clasificación.\n"
      ],
      "metadata": {
        "id": "Ubh9BqBi3g-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importaciones"
      ],
      "metadata": {
        "id": "AZk6paD04pWD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzLKpmZICaWN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from IPython import display\n",
        "import tensorflow_datasets as tfds\n",
        "from IPython.display import Audio, display as ipy_display\n",
        "\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar para que TensorFlow utilice la GPU por defecto\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Configurar para que TensorFlow asigne memoria dinámicamente\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        # Especificar la GPU por defecto\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Manejar error\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "g_uDfXfJOXp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga de datos"
      ],
      "metadata": {
        "id": "oG06qRci3o2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el dataset con división 80% train y 20% validación\n",
        "(df_train, df_val), df_info = tfds.load('spoken_digit', split=['train[:80%]', 'train[80%:]'], with_info=True, as_supervised=True)"
      ],
      "metadata": {
        "id": "FFfxdiJi9mTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener nombres de clases\n",
        "label_names = np.array(df_info.features[\"label\"].names)\n",
        "print(\"Clases disponibles:\", label_names)\n",
        "\n",
        "# Mostrar cantidad real de ejemplos en cada subset\n",
        "print(\"Ejemplos en df_train:\", tf.data.experimental.cardinality(df_train).numpy())\n",
        "print(\"Ejemplos en df_val:\",   tf.data.experimental.cardinality(df_val).numpy())\n",
        "\n",
        "# Mostrar ejemplos de df_train\n",
        "for audio, label in df_train.take(3):\n",
        "    print(f\"Label: {label.numpy()} ({label_names[label.numpy()]})\")\n",
        "    print(f\"Forma del audio: {audio.shape}\")"
      ],
      "metadata": {
        "id": "RfqPKdhqyw0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocesamiento y Análisis de datos (EDA)"
      ],
      "metadata": {
        "id": "FgEVWdN23rtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# recorremos los audios y creamos un DataFrame con su información\n",
        "\n",
        "audios = []\n",
        "labels = []\n",
        "\n",
        "for audio, label in df_train:\n",
        "    audios.append(audio.numpy())\n",
        "    labels.append(label.numpy())\n",
        "\n",
        "data = []\n",
        "\n",
        "for audio, label in df_train:\n",
        "    arr = audio.numpy()\n",
        "    data.append({\n",
        "        'clase': label.numpy(),\n",
        "        'duracion_muestras': len(arr),\n",
        "        'media': np.mean(arr),\n",
        "        'std': np.std(arr),\n",
        "        'max': np.max(arr),\n",
        "        'min': np.min(arr)\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "ge71pwOcm-RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "_jYUVlG8nK6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Datos obtenidos hasta ahora:\n",
        "\n",
        "\n",
        "*   hay 9 clases disponibles\n",
        "*   hay 2000 datos de train y 500 de validación\n",
        "*   duración de las muestras de audio\n",
        "*   mín y max son métricas del array de numpy para ese audio\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0EG5X3cfncOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribución de clases\n",
        "sns.countplot(x='clase', data=df)\n",
        "plt.title(\"Distribución de clases\")\n",
        "plt.show()\n",
        "\n",
        "# Distribución de duración\n",
        "sns.histplot(df['duracion_muestras'], bins=30, kde=True)\n",
        "plt.title(\"Distribución de duraciones\")\n",
        "plt.show()\n",
        "\n",
        "# Boxplot por clase\n",
        "sns.boxplot(x='clase', y='duracion_muestras', data=df)\n",
        "plt.title(\"Duración por clase\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mizJG6bC_jgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusión:\n",
        "\n",
        "Distribución de clases (Gráfico 1):\n",
        "\n",
        "Muestra una distribución medianamente balanceada de las clases (0 a 9). Confirmamos que no hay gran desbalance en la cantidad de muestras por clase.\n",
        "\n",
        "Distribución de duraciones (Gráfico 2):\n",
        "\n",
        "Podemos observar la distribución general de la duración de los audios. Vemos una asimetría positiva (cola hacia la derecha), la distribución se asemeja a una normal.\n",
        "\n",
        "Boxplot por clase (Gráfico 3):\n",
        "\n",
        "Nos permite ver la mediana, los cuartiles y los posibles outliers de duración por cada dígito.\n",
        "Detectamos si hay clases con duraciones anómalas o diferentes en promedio.\n"
      ],
      "metadata": {
        "id": "i0DrJsZK279Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar y reproducir los primeros 3 audios\n",
        "for i, (audio, label) in enumerate(df_train.take(3)):\n",
        "    print(f\"Ejemplo {i+1} - Dígito: {label.numpy()}\")\n",
        "    ipy_display(Audio(audio.numpy(), rate=16000))  # Asumimos sample rate de 16kHz"
      ],
      "metadata": {
        "id": "0uk4NriN_0v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar y reproducir los primeros 3 audios\n",
        "for i, (audio, label) in enumerate(df_val.take(3)):\n",
        "    print(f\"Ejemplo {i+1} - Dígito: {label.numpy()}\")\n",
        "    ipy_display(Audio(audio.numpy(), rate=16000))  # Asumimos sample rate de 16kHz"
      ],
      "metadata": {
        "id": "I4kxQkWkiEjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Squeeze\n",
        "Para normalizar y llevar todos los audios a 8000hz (en la página de tensorflow dice que todos están en esa frecuencia)"
      ],
      "metadata": {
        "id": "jsy6Bzz1xz48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LONG_HZ = 8000  # 1 segundo a 8kHz\n",
        "\n",
        "def squeeze(audio, label):\n",
        "    \"\"\"\n",
        "    Normaliza la señal de audio, elimina dimensiones extra y ajusta su longitud a LONG_HZ.\n",
        "    - Convierte a float32 en [-1, 1]\n",
        "    - Quita cualquier dimensión extra (como [N,1] -> [N])\n",
        "    - Si el audio es más corto, hace padding con ceros.\n",
        "    - Si es más largo, lo recorta a LONG_HZ.\n",
        "    \"\"\"\n",
        "    # 1) Convertir a float32 y normalizar\n",
        "    audio = tf.cast(audio, tf.float32) / 32768.0\n",
        "\n",
        "    # 2) Asegurar forma 1D\n",
        "    audio = tf.reshape(audio, [-1])  # más robusto que tf.squeeze\n",
        "\n",
        "    # 3) Ajustar la longitud\n",
        "    audio_len = tf.shape(audio)[0]\n",
        "    audio = tf.cond(\n",
        "        audio_len < LONG_HZ,\n",
        "        lambda: tf.pad(audio, [[0, LONG_HZ - audio_len]]),\n",
        "        lambda: audio[:LONG_HZ]\n",
        "    )\n",
        "\n",
        "    return audio, label"
      ],
      "metadata": {
        "id": "lM6vdUbqyASe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.map(squeeze, num_parallel_calls=AUTOTUNE)\n",
        "df_val = df_val.map(squeeze, num_parallel_calls=AUTOTUNE)"
      ],
      "metadata": {
        "id": "kuXR3C_O1you"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for audio, label in df_train.take(1):\n",
        "    print(\"Forma del audio:\", audio.shape)  # (8000,)\n",
        "    print(\"Etiqueta:\", label.numpy())"
      ],
      "metadata": {
        "id": "X4qmrsVd2B_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotear forma de onda para algunos ejemplos\n",
        "\n",
        "plt.figure(figsize=(16, 10))\n",
        "rows = 3\n",
        "cols = 3\n",
        "n = rows * cols\n",
        "\n",
        "audios = []\n",
        "labels = []\n",
        "\n",
        "for audio, label in df_train.take(n):\n",
        "    audios.append(audio.numpy())\n",
        "    labels.append(label.numpy())\n",
        "\n",
        "for i in range(n):\n",
        "    plt.subplot(rows, cols, i+1)\n",
        "    plt.plot(audios[i])\n",
        "    plt.title(label_names[labels[i]])\n",
        "    plt.yticks(np.arange(-1.2, 1.2, 0.2))\n",
        "    plt.ylim([-1.1, 1.1])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Up2JFBtp5SwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convertir a espectogramas para modelo convolucional\n",
        "\n",
        "se transforman las formas de onda de **señales del dominio del tiempo** a **señales del dominio tiempo-frecuencia** al calcular la transformada de Fourier de corto tiempo (STFT) para convertir las formas de onda en espectrogramas, que muestran cambios de frecuencia a lo largo del tiempo y pueden ser representados como imágenes 2D."
      ],
      "metadata": {
        "id": "FbvFBoWN6gbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_spectrogram(waveform):\n",
        "  # Convierte la forma de onda en espectograma mediante STFT\n",
        "  spectrogram = tf.signal.stft(\n",
        "      waveform, frame_length=255, frame_step=128)\n",
        "  # Obtiene la maginitud de STFT\n",
        "  spectrogram = tf.abs(spectrogram)\n",
        "  # agrega un canal a la dimensión para que el espectograma se pueda usar\n",
        "  # as image-like input data with convolution layers (which expect\n",
        "  # shape (`batch_size`, `height`, `width`, `channels`).\n",
        "  spectrogram = spectrogram[..., tf.newaxis]\n",
        "  return spectrogram"
      ],
      "metadata": {
        "id": "uQjrUtYK6gFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_spectrogram(spectrogram, ax):\n",
        "    if len(spectrogram.shape) > 2:\n",
        "        spectrogram = np.squeeze(spectrogram, axis=-1)\n",
        "    # logaritmo para mejor visualización\n",
        "    log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n",
        "    height = log_spec.shape[0]\n",
        "    width = log_spec.shape[1]\n",
        "    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n",
        "    Y = range(height)\n",
        "    ax.pcolormesh(X, Y, log_spec, shading='gouraud')"
      ],
      "metadata": {
        "id": "wA0DLKG079YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tomar 3 ejemplos del dataset procesado (normalizado y con longitud 8000)\n",
        "example_audios = []\n",
        "example_labels = []\n",
        "for audio, label in df_train.take(3):\n",
        "    example_audios.append(audio)\n",
        "    example_labels.append(label.numpy())"
      ],
      "metadata": {
        "id": "AjS-ff3l8vQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización\n",
        "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
        "\n",
        "for i in range(3):\n",
        "    waveform = example_audios[i]\n",
        "    label = label_names[example_labels[i]]\n",
        "    spectrogram = get_spectrogram(waveform)\n",
        "\n",
        "    # Forma de onda\n",
        "    timescale = np.arange(waveform.shape[0])\n",
        "    axes[i, 0].plot(timescale, waveform.numpy())\n",
        "    axes[i, 0].set_title(f'Waveform - Label: {label}')\n",
        "    axes[i, 0].set_xlim([0, LONG_HZ])\n",
        "    axes[i, 0].set_ylim([-1.1, 1.1])\n",
        "\n",
        "    # Espectrograma\n",
        "    plot_spectrogram(spectrogram.numpy(), axes[i, 1])\n",
        "    axes[i, 1].set_title('Spectrogram')\n",
        "\n",
        "    # Audio playback (solo del primer ejemplo para no saturar)\n",
        "    if i == 0:\n",
        "        ipy_display(display.Audio(waveform.numpy(), rate=LONG_HZ))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5q8CZwCB8zLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crear datasets con los espectogramas"
      ],
      "metadata": {
        "id": "jhaZutTq9cP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_spec_ds(ds):\n",
        "  return ds.map(\n",
        "      map_func=lambda audio,label: (get_spectrogram(audio), label),\n",
        "      num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "MBuoAvRi9fUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "train_spectrogram_ds = make_spec_ds(df_train).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "val_spectrogram_ds = make_spec_ds(df_val).batch(BATCH_SIZE).prefetch(AUTOTUNE)"
      ],
      "metadata": {
        "id": "yoq9FvqC9knl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examinar espectogramas para los distintos ejemplos del dataset\n",
        "for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n",
        "  break"
      ],
      "metadata": {
        "id": "UeKIB4ks9pDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = 3\n",
        "cols = 3\n",
        "n = rows*cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n",
        "\n",
        "for i in range(n):\n",
        "    r = i // cols\n",
        "    c = i % cols\n",
        "    ax = axes[r][c]\n",
        "    plot_spectrogram(example_spectrograms[i].numpy(), ax)\n",
        "    ax.set_title(label_names[example_spect_labels[i].numpy()])"
      ],
      "metadata": {
        "id": "_XXzlMaw9tnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo convolucional\n",
        "\n",
        "TRABAJA CON IMÁGENES 2D (ESPECTOGRAMAS)"
      ],
      "metadata": {
        "id": "OiG9PRCS3wcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizar la carga del dataset"
      ],
      "metadata": {
        "id": "pI46hxBL-qyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
        "val_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "6gtqtHus-uYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definición del modelo"
      ],
      "metadata": {
        "id": "pG6TaGhi366B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = example_spectrograms.shape[1:]\n",
        "print('Input shape:', input_shape)\n",
        "num_labels = len(label_names)\n",
        "\n",
        "# Instantiate the `tf.keras.layers.Normalization` layer.\n",
        "norm_layer = layers.Normalization()\n",
        "# Fit the state of the layer to the spectrograms\n",
        "# with `Normalization.adapt`.\n",
        "norm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    # Downsample the input.\n",
        "    layers.Resizing(32, 32),\n",
        "    # Normalize.\n",
        "    norm_layer,\n",
        "    layers.Conv2D(32, 3, activation='relu'),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels),\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "uZG6MWzl4kAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilación\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ],
      "metadata": {
        "id": "U1tHlLSl6QCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento"
      ],
      "metadata": {
        "id": "4g_6h_R63-DZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "history = model.fit(\n",
        "    train_spectrogram_ds,\n",
        "    validation_data=val_spectrogram_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=5),\n",
        ")"
      ],
      "metadata": {
        "id": "HDRi4pvR4kXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluación"
      ],
      "metadata": {
        "id": "40Ham8_X4AIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización\n",
        "metrics = history.history\n",
        "plt.figure(figsize=(16,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss [CrossEntropy]')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\n",
        "plt.legend(['accuracy', 'val_accuracy'])\n",
        "plt.ylim([0, 100])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy [%]')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TUe8qdtZ4ksQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# matriz de confusión para veificar qué tan bien clasificó el modelo\n",
        "y_pred = model.predict(val_spectrogram_ds)\n",
        "y_pred = tf.argmax(y_pred, axis=1)\n",
        "y_true = tf.concat(list(val_spectrogram_ds.map(lambda s,lab: lab)), axis=0)\n",
        "\n",
        "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx,\n",
        "            xticklabels=label_names,\n",
        "            yticklabels=label_names,\n",
        "            annot=True, fmt='g')\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LZq3-mfG_lTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inferir sobre un archivo de audio"
      ],
      "metadata": {
        "id": "Lw3VG4vcAgh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# debería usar el conjunto de val\n",
        "# pasar por squeeze\n",
        "# convertir a espectograma\n",
        "# y por ultimo inferir en el modelo a ver que tal funciona"
      ],
      "metadata": {
        "id": "FRE43lfaAdJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predecir_y_escuchar(dataset, index=0):\n",
        "  for i, (waveform, label) in enumerate(dataset.skip(index).take(1)):\n",
        "    spectrogram = get_spectrogram(waveform)\n",
        "    spectrogram = spectrogram[tf.newaxis, ...]\n",
        "    prediction = model(spectrogram)\n",
        "\n",
        "    x_labels = label_names\n",
        "    plt.bar(x_labels, tf.nn.softmax(prediction[0]))\n",
        "    plt.title(f'Prediction - Label: {label_names[label.numpy()]}')\n",
        "    plt.show()\n",
        "\n",
        "    display.display(display.Audio(waveform, rate=16000))\n",
        "    print(f'Etiqueta real: {label_names[label.numpy()]}')\n",
        "\n",
        "predecir_y_escuchar(df_val, index=5)"
      ],
      "metadata": {
        "id": "lMhDzNHsyCLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo recurrente\n",
        "\n",
        "(TRABAJA CON VECTORES)"
      ],
      "metadata": {
        "id": "Uv9Qe9ls3zLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparación del dataset\n",
        "\n",
        "Sacamos el canal que no es necesario para este modelo"
      ],
      "metadata": {
        "id": "VTBPLLyFBoe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_channel(spectrogram, label):\n",
        "    spectrogram = tf.squeeze(spectrogram, axis=-1)  # quita el canal de tamaño 1\n",
        "    return spectrogram, label"
      ],
      "metadata": {
        "id": "cK5LT-wMDsAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_rnn = train_spectrogram_ds.map(remove_channel)\n",
        "val_rnn = val_spectrogram_ds.map(remove_channel)"
      ],
      "metadata": {
        "id": "4LHdRQkjBzyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Revisar la forma de un batch\n",
        "for batch_x, batch_y in train_rnn.take(1):\n",
        "    print(batch_x.shape)  # debería ser (BATCH_SIZE, timesteps, features)"
      ],
      "metadata": {
        "id": "GnQBmxFKD0Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizar la carga del Dataset"
      ],
      "metadata": {
        "id": "e0cRi-5d-EyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_rnn = train_rnn.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
        "val_rnn = val_rnn.cache().prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "ABhhxu0T-NDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definición del modelo"
      ],
      "metadata": {
        "id": "8Xl4oLPQ4E2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalización - adaptá usando muestras del dataset sin canal\n",
        "batch_specs = []\n",
        "for spec, _ in train_rnn.take(100):  # spec shape: (batch_size, timesteps, features)\n",
        "    batch_specs.append(spec.numpy())\n",
        "batch_specs = np.concatenate(batch_specs, axis=0)  # concatena todos los batchs en una sola dimensión de samples\n",
        "\n",
        "norm_layer = layers.Normalization()\n",
        "norm_layer.adapt(batch_specs)\n",
        "\n",
        "# Input shape\n",
        "example_spectrograms = next(iter(train_rnn.take(1)))[0]  # (batch_size, timesteps, features)\n",
        "input_shape_rnn = example_spectrograms.shape[1:]        # (timesteps, features)\n",
        "\n",
        "# Modelo\n",
        "rnn_model = models.Sequential([\n",
        "    layers.Input(shape=input_shape_rnn),\n",
        "    norm_layer,\n",
        "    layers.LSTM(64, return_sequences=True),\n",
        "    layers.LSTM(64),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(num_labels),\n",
        "])"
      ],
      "metadata": {
        "id": "iJC4h5nQ-Tm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilación\n",
        "rnn_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "7DxOSPfK-cqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento"
      ],
      "metadata": {
        "id": "SpKgZdq94E2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento\n",
        "EPOCHS = 50\n",
        "history_rnn = rnn_model.fit(\n",
        "    train_rnn,\n",
        "    validation_data=val_rnn,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=5),\n",
        ")"
      ],
      "metadata": {
        "id": "egwRBftb-dU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluación"
      ],
      "metadata": {
        "id": "-Cnkj97D4E2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluación\n",
        "metrics = history_rnn.history\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "\n",
        "# Gráfico de pérdida (loss)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_rnn.epoch, metrics['loss'], label='Training Loss')\n",
        "plt.plot(history_rnn.epoch, metrics['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.ylim([0, max(max(metrics['loss']), max(metrics['val_loss']))])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss [CrossEntropy]')\n",
        "plt.title('Loss vs Epochs')\n",
        "\n",
        "# Gráfico de precisión (accuracy)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_rnn.epoch, 100 * np.array(metrics['accuracy']), label='Training Accuracy')\n",
        "plt.plot(history_rnn.epoch, 100 * np.array(metrics['val_accuracy']), label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.ylim([0, 100])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy [%]')\n",
        "plt.title('Accuracy vs Epochs RNN')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FVhPOi8M-fRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "y_pred_rnn = rnn_model.predict(val_rnn)\n",
        "y_pred_rnn = tf.argmax(y_pred_rnn, axis=1)\n",
        "y_true_rnn = tf.concat(list(val_rnn.map(lambda s, lab: lab)), axis=0)\n",
        "\n",
        "confusion_mtx = tf.math.confusion_matrix(y_true_rnn, y_pred_rnn)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_mtx,\n",
        "            xticklabels=label_names,\n",
        "            yticklabels=label_names,\n",
        "            annot=True, fmt='g')\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Is3Rhyr2-jDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inferir sobre un archivo de audio"
      ],
      "metadata": {
        "id": "VRPT93iYEmGJ"
      }
    }
  ]
}